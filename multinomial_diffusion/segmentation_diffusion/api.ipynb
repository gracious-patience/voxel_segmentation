{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed to work out internal functions for MLP classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all relative imports to work fine\n",
    "import sys\n",
    "sys.path.append(\"/home/sharfikeg/my_files/multinomial_diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from model import get_model, get_model_id, add_model_args\n",
    "from layers.layers import SegmentationUnet\n",
    "from diffusion_utils.diffusion_multinomial import MultinomialDiffusion\n",
    "from diffusion_utils.diffusion_multinomial import index_to_log_onehot,log_onehot_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharfikeg/anaconda3/envs/voxel/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input\n",
    "array = numpy.load(\n",
    "    \"/home/sharfikeg/my_files/VoxelDiffusion/data/ShapeNet15k_voxels/dishwasher/5d17e90f512a3dc7df3a1b0d597ce76e.npy\"\n",
    ")\n",
    "x = torch.tensor(array).unsqueeze(0).long().to('cuda')\n",
    "time = torch.tensor([200]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models: UNet and Diffusion \n",
    "# UNet is used to gather activations\n",
    "# Diffusion is used to get noisy samples\n",
    "model = SegmentationUnet(\n",
    "    num_classes=2,\n",
    "    dim=32,\n",
    "    num_steps=1000,\n",
    "    dim_mults = (1, 8, 16, 32)\n",
    ")\n",
    "\n",
    "base_dist = MultinomialDiffusion(\n",
    "    num_classes=2, shape=(1,32,32,32), denoise_fn=model, timesteps=1000\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example pretrained model loading\n",
    "dict = torch.load(\"/home/sharfikeg/my_files/multinomial_diffusion/log/flow/shape_net_chairs/multinomial_diffusion/multistep/2023-03-15_16-21-31/check/checkpoint.pt\")\n",
    "base_dist.load_state_dict(dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines to \n",
    "# 1) convert input tensor to the necessary format\n",
    "# 2) get noisy tensor\n",
    "# 3) convert input tensor back to the format, suitable for UNet\n",
    "log_x_start = index_to_log_onehot(x, num_classes=2)\n",
    "log_x_t = base_dist.q_sample(log_x_start=log_x_start, t=time)\n",
    "x_t = log_onehot_to_index(log_x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage of UNet\n",
    "base_dist._denoise_fn(\n",
    "    time=time,\n",
    "    x=x_t\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here: work with Dima's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.feature_extractor\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DDPM Feature Extractor...\n",
      "Pretrained model is successfully loaded from /home/sharfikeg/my_files/multinomial_diffusion/log/flow/shape_net_chairs/multinomial_diffusion/multistep/2023-03-15_16-21-31/check/checkpoint.pt\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# feature extractor from Dima's\n",
    "model_path = \"/home/sharfikeg/my_files/multinomial_diffusion/log/flow/shape_net_chairs/multinomial_diffusion/multistep/2023-03-15_16-21-31/check/checkpoint.pt\"\n",
    "exctractor = src.feature_extractor.create_feature_extractor(\n",
    "    model_type='ddpm',\n",
    "    model_path=model_path,\n",
    "    input_activations=False,\n",
    "    num_classes=2,\n",
    "    num_steps=1000,\n",
    "    dim=32,\n",
    "    shape=(1,32,32,32),\n",
    "    dim_mults=(1,8,16,32),\n",
    "    steps=[100],\n",
    "    blocks=[0,1,2],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1824, 32, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.feature_extractor.collect_features({\"shape\":(1,32,32,32), \"upsample_mode\":\"trilinear\"},exctractor(x, None)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 4, 4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exctractor(x, None)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import blobfile as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_voxel_files_recursively(data_dir):\n",
    "    results = []\n",
    "    for entry in sorted(bf.listdir(data_dir)):\n",
    "        full_path = bf.join(data_dir, entry)\n",
    "        ext = entry.split(\".\")[-1]\n",
    "        if \".\" in entry and ext.lower() in [\"npy\"]:\n",
    "            results.append(full_path)\n",
    "        elif bf.isdir(full_path):\n",
    "            results.extend(_list_voxel_files_recursively(full_path))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLabelDataset(Dataset):\n",
    "    ''' \n",
    "    :param data_dir: path to a folder with images and their annotations. \n",
    "                     Annotations are supposed to be in *.npy format.\n",
    "    :param resolution: image and mask output resolution.\n",
    "    :param num_images: restrict a number of images in the dataset.\n",
    "    :param transform: image transforms.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        resolution: int,\n",
    "        num_images= -1,\n",
    "        transform=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.resolution = resolution\n",
    "        self.transform = transform\n",
    "        self.image_paths = _list_voxel_files_recursively(data_dir)\n",
    "        self.image_paths = sorted(self.image_paths)\n",
    "\n",
    "        if num_images > 0:\n",
    "            print(f\"Take first {num_images} images...\")\n",
    "            self.image_paths = self.image_paths[:num_images]\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load an image\n",
    "        image_path = self.image_paths[idx]\n",
    "        object_and_mask = numpy.load(image_path)\n",
    "        \n",
    "\n",
    "        # assert pil_image.size[0] == pil_image.size[1], \\\n",
    "        #        f\"Only square images are supported: ({pil_image.size[0]}, {pil_image.size[1]})\"\n",
    "\n",
    "        tensor_object_and_mask = self.transform(object_and_mask)\n",
    "        tensor_object = tensor_object_and_mask[0].unsqueeze(0)\n",
    "        # Load a corresponding mask and resize it to (self.resolution, self.resolution)\n",
    "        # label_path = self.label_paths[idx]\n",
    "        # label = np.load(label_path).astype('uint8')\n",
    "        # label = cv2.resize(\n",
    "        #     label, (self.resolution, self.resolution), interpolation=cv2.INTER_NEAREST\n",
    "        # )\n",
    "        tensor_label = tensor_object_and_mask[1:,...]\n",
    "        return tensor_object, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take first 10 images...\n"
     ]
    }
   ],
   "source": [
    "d = ImageLabelDataset(\n",
    "    \"/home/sharfikeg/my_files/VoxelDiffusion/data/ShapeNet_annotated_voxels/chair\",\n",
    "    32,\n",
    "    10,\n",
    "    lambda array: (torch.FloatTensor(array)).long()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for data in d:\n",
    "    print(data[0].shape[:-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for MLPs training\n",
    "def prepare_data(args):\n",
    "    feature_extractor = src.feature_extractor.create_feature_extractor(**args)\n",
    "    \n",
    "    print(f\"Preparing the train set for ...\")\n",
    "    dataset = ImageLabelDataset(\n",
    "        data_dir=args['training_path'],\n",
    "        resolution=args['image_size'],\n",
    "        num_images=args['training_number'],\n",
    "        transform=lambda array: (torch.FloatTensor(array)).long()\n",
    "    )\n",
    "    X = torch.zeros((len(dataset),args['num_of_features'] , *args['shape'][1:]), dtype=torch.uint8)\n",
    "    y = torch.zeros((len(dataset), *args['shape'][1:]), dtype=torch.uint8)\n",
    "\n",
    "    if 'share_noise' in args and args['share_noise']:\n",
    "        rnd_gen = torch.Generator(device=args['device']).manual_seed(args['seed'])\n",
    "        noise = torch.randn(1, 3, args['image_size'], args['image_size'], \n",
    "                            generator=rnd_gen, device=args['device'])\n",
    "    else:\n",
    "        noise = None \n",
    "\n",
    "    for row, (img, label) in enumerate(tqdm(dataset)):\n",
    "        img = img[None].to(args['device'])\n",
    "        features = feature_extractor(img, noise=noise)\n",
    "        X[row] = src.feature_extractor.collect_features(args, features).cpu()\n",
    "        \n",
    "        # for target in range(args['number_class']):\n",
    "        #     if target == args['ignore_label']: continue\n",
    "        #     if 0 < (label == target).sum() < 20:\n",
    "        #         print(f'Delete small annotation from image {dataset.image_paths[row]} | label {target}')\n",
    "        #         label[label == target] = args['ignore_label']\n",
    "        y[row] = label.argmax(0)\n",
    "    \n",
    "    d = X.shape[1]\n",
    "    print(f'Total dimension {d}')\n",
    "    X = X.permute(1,0,2,3,4).reshape(d, -1).permute(1, 0)\n",
    "    y = y.flatten()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/sharfikeg/my_files/multinomial_diffusion/log/flow/shape_net_chairs/multinomial_diffusion/multistep/2023-03-09_10-47-15/check/checkpoint.pt\"\n",
    "exp_path = \"/home/sharfikeg/my_files/multinomial_diffusion/segmentation_diffusion/exp_dir\"\n",
    "args = {\n",
    "    'model_type':'ddpm',\n",
    "    'model_path':model_path,\n",
    "    'input_activations':False,\n",
    "    'num_classes':2,\n",
    "    'num_steps':1000,\n",
    "    'dim':32,\n",
    "    'shape':(1,32,32,32),\n",
    "    'num_of_features':224*2,\n",
    "    'steps':[250, 300],\n",
    "    'blocks':[0,1,2],\n",
    "    'training_path':\"/home/sharfikeg/my_files/VoxelDiffusion/data/ShapeNet_annotated_voxels/chair\",\n",
    "    'image_size':32,\n",
    "    'training_number':3010,\n",
    "    'share_noise':False,\n",
    "    'device': 'cuda',\n",
    "    'number_class': 5,\n",
    "    'upsample_mode':\"trilinear\",\n",
    "    'start_model_num': 0,\n",
    "    'model_num': 1,\n",
    "    'exp_dir': exp_path,\n",
    "    'batch_size': 16,\n",
    "    }\n",
    "\n",
    "\n",
    "X,y = prepare_data(\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_mlp import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/sharfikeg/my_files/multinomial_diffusion/log/flow/shape_net_chairs/multinomial_diffusion/multistep/2023-03-09_10-47-15/check/checkpoint.pt\"\n",
    "exp_path = \"/home/sharfikeg/my_files/multinomial_diffusion/segmentation_diffusion/exp_dir\"\n",
    "args = {\n",
    "    'model_type':'ddpm',\n",
    "    'model_path':model_path,\n",
    "    'input_activations':False,\n",
    "    'num_classes':2,\n",
    "    'num_steps':1000,\n",
    "    'dim':32,\n",
    "    'shape':(1,32,32,32),\n",
    "    'num_of_features':224*2,\n",
    "    'steps':[250, 300],\n",
    "    'blocks':[0,1,2],\n",
    "    'training_path':\"/home/sharfikeg/my_files/VoxelDiffusion/data/ShapeNet_annotated_voxels/chair\",\n",
    "    'image_size':32,\n",
    "    'training_number':10,\n",
    "    'share_noise':False,\n",
    "    'device': 'cuda',\n",
    "    'number_class': 5,\n",
    "    'upsample_mode':\"trilinear\",\n",
    "    'start_model_num': 0,\n",
    "    'model_num': 1,\n",
    "    'ignore_label': 1,\n",
    "    'exp_dir': exp_path,\n",
    "    'batch_size': 16,\n",
    "    }\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cbdfa006dadc479671f0f739dc517dd49e91250c5f0048d4b70849465bd293b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
